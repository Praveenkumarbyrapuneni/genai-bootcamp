# ğŸ¤– RAG Chatbot using FAISS + Ollama

A fully local **Retrieval-Augmented Generation (RAG)** chatbot built step-by-step using **FAISS**, **Sentence Transformers**, **Ollama (LLaMA)**, and **Streamlit**.  
This project demonstrates a complete GenAI pipeline â€” from document ingestion to a production-ready chat UI â€” with hybrid intelligence (private documents + general LLM knowledge).

---

## ğŸš€ Project Overview

This chatbot answers user questions by:
- Using **your own documents** when relevant (RAG)
- Falling back to **general LLM knowledge** when documents donâ€™t contain the answer
- Showing **source citations** whenever document-based answers are used

Everything runs **locally**. No paid APIs.

---

## ğŸ§  What is RAG?

**Retrieval-Augmented Generation (RAG)** improves LLM answers by grounding them in external documents.

Flow:
User Question â†’ Embedding â†’ FAISS Vector Search â†’ Relevant Chunks â†’ LLM â†’ Final Answer

---

## âœ¨ Key Features

- Local document ingestion (`data.txt`)
- Text chunking with overlap
- Embeddings using `all-MiniLM-L6-v2`
- FAISS vector similarity search
- Local LLM inference with Ollama (LLaMA)
- Hybrid answers (RAG + general knowledge)
- Source citations
- Conversational memory
- Streamlit interactive UI
- Modular learning-friendly file structure

---

## ğŸ“ Project Structure

rag-chatbot-faiss-ollama/
â”œâ”€â”€ data.txt
â”œâ”€â”€ 01_app_chunks.py
â”œâ”€â”€ 02_app_embeddings.py
â”œâ”€â”€ 03_app_faiss_indexing.py
â”œâ”€â”€ 04_app_top_chunks_retrieval.py
â”œâ”€â”€ 05_app_generation_ollama.py
â”œâ”€â”€ 06_app_pipeline_fixed_questions.py
â”œâ”€â”€ 07_RAG_loop_app.py
â”œâ”€â”€ 08_rag_with_citations_app.py
â”œâ”€â”€ 09_rag_with_memory_app.py
â”œâ”€â”€ 10_streamlit_rag_app.py
â”œâ”€â”€ 11_streamlit_rag_plus_general_chat_app.py
â””â”€â”€ README.md


---

## ğŸ› ï¸ Tech Stack

- Python
- FAISS
- Sentence Transformers
- Ollama (LLaMA models)
- Streamlit
- LangChain Text Splitters

---

## âš™ï¸ Setup Instructions

Create virtual environment:
python -m venv rag-env
source rag-env/bin/activate


Install dependencies:
pip install faiss-cpu sentence-transformers langchain streamlit


Install Ollama and model:
brew install ollama
ollama pull llama3.2:3b


---

## â–¶ï¸ How to Run

Run CLI RAG loop:
python 07_RAG_loop_app.py


Run Streamlit UI:
streamlit run 11_streamlit_rag_plus_general_chat_app.py

Open browser:
http://localhost:8501


---

## ğŸ§ª Example Behavior

- If the answer exists in `data.txt` â†’ uses retrieved chunks + shows sources
- If not â†’ answers from general LLM knowledge (no fake citations)

---

## ğŸ¯ Learning Outcomes

- Built an end-to-end RAG system
- Learned embeddings and vector databases
- Implemented FAISS similarity search
- Controlled hallucinations
- Deployed local LLMs using Ollama
- Built a GenAI web app using Streamlit
- Structured a real-world GenAI project

---

## ğŸš§ Future Improvements

- PDF / multi-file ingestion
- Persistent FAISS index
- User authentication
- Docker deployment
- Cloud vector DB support

---

## ğŸ‘¤ Author

**Praveen Kumar Byrapuneni**  
GenAI Bootcamp Participant  

GitHub: https://github.com/Praveenkumarbyrapuneni

---

â­ If you found this project useful, please star the repository!
